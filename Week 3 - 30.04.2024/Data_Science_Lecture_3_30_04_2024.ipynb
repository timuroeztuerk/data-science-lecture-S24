{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization and Bag-of-Words\n",
        "\n",
        "Welcome to week three. This week, we will build on what we learned last week. Regular expressions were helpful to us, since it will be the main way to clean the string (text) data. Now we will look at the elementary applications, once we have a clean data."
      ],
      "metadata": {
        "id": "skbw7blZiXk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From last week you will remember our short introduction to tokenization. Let's recap what we have learned and work on Adam Smith's Wealth of Nations to understand the text a little better."
      ],
      "metadata": {
        "id": "GulVqKY8o1Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Short (Re)Introduction to Tokenization\n",
        "# The NLTK library we imported above gives us the ability to \"tokenize\" pieces of text into smaller pieces. These tokens can be thought of how we keep words in our minds, but for the computer.\n",
        "\n",
        "example_text = \"This is the Data Science Lecture. We are going to have so much fun! The fun will not end...\"\n",
        "\n",
        "# Now, we can put this example text into sentence tokenization. This will parse the sentences from the text.\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "first_result = sent_tokenize(example_text)\n",
        "first_result"
      ],
      "metadata": {
        "id": "Wz6A2hmM7rwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Great! But we might also be interested in words, rather than sentences. No worries, NLTK has a solution for that as well.\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "second_result = word_tokenize(example_text)\n",
        "second_result"
      ],
      "metadata": {
        "id": "KcVYeaq0Rpvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It did what we asked it to do, but there are clear problems, there are two instances of 'the', one capitalized.\n",
        "# To fix this problem, we can simply convert everything to lowercase, which is a common method in the field.\n",
        "second_result_lower = [token.lower() for token in second_result]\n",
        "second_result_lower"
      ],
      "metadata": {
        "id": "W4MOraeBSGyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eKnmmqyiSBN"
      },
      "outputs": [],
      "source": [
        "# The resources we will use this lecture.\n",
        "wealth_of_nations = \"https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/The_Wealth_of_Nations_Volume_1_Cleaned.txt\"\n",
        "# Importing the necessary libraries we have used in the last lectures.\n",
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt') # Some extra knowledge for the computer, so it knows where the sentences are.\n",
        "nltk.download('stopwords') # Same, but this time it will know the english stopwords like 'the' and 'and'.\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the book from Adam Smith.\n",
        "adam_smith = requests.get(wealth_of_nations).text"
      ],
      "metadata": {
        "id": "8yM2Zoyu2DtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will do three iterations of this example. First, let's simply tokenize the text using the simplest version of the nltk command.\n",
        "# Import the tokenization command.\n",
        "from  nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text.\n",
        "unique_tokens = word_tokenize(adam_smith)\n",
        "\n",
        "# Lowercase all tokens so that there are no confusions. (i.e. 'The' vs. 'the')\n",
        "lower_tokens = [token.lower() for token in unique_tokens]\n",
        "\n",
        "# Let's count what we have in this text. For this, we will use a basic Python counter. Import the counter first.\n",
        "from collections import Counter\n",
        "\n",
        "# Call the counter on our unique_tokens.\n",
        "wealth_counts = Counter(unique_tokens)\n",
        "\n",
        "# What are the most common tokens?\n",
        "wealth_counts.most_common(5)\n",
        "\n",
        "# The results are interesting. Appearently the most common token in Wealth of Nations is a comma."
      ],
      "metadata": {
        "id": "abRupS_qxTGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we get around this problem? No worries, the python community has a fix for this.\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# You remember our variable lower_tokens from above? Let's make a quick adjustment by filtering out some stopwords.\n",
        "# Have a carfeul look at the loop below, and try to understand what it does.\n",
        "filtered_words = [token for token in lower_tokens if not token in stopwords.words('english')]\n",
        "\n",
        "# Going back to counting and listing the most common words:\n",
        "wealth_counts_filtered = Counter(filtered_words)\n",
        "wealth_counts_filtered.most_common(5)\n",
        "\n",
        "# Okay, this is better, but we just got rid of the stopwords. We still have punctuation to deal with."
      ],
      "metadata": {
        "id": "ENLKkcWb2eno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a step back, and use another version of the tokenizer from NLTK library.\n",
        "# If you are confused, look at two cells above. Instead of word_tokenize() we will use RegexpTokenizer().\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Create your own version of regular expression tokenizer. Here we specify that it should only look for words.\n",
        "timur = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Let's do the processing.\n",
        "words = timur.tokenize(adam_smith)\n",
        "\n",
        "# Same filter for stopwords as above.\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "# Okay, one last time counting it all, let's see what we have now.\n",
        "wealth_counts_filtered = Counter(filtered_words)\n",
        "wealth_counts_filtered.most_common(20)\n",
        "\n",
        "# Can we somehow make this better?"
      ],
      "metadata": {
        "id": "BS2k_XgQ3hjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Classic lowercase tokenization.\n",
        "unique_tokens = word_tokenize(adam_smith)\n",
        "lower_tokens = [token.lower() for token in unique_tokens]\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(20))"
      ],
      "metadata": {
        "id": "7h74vRnE9zEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment 1: Using 'https://www.gutenberg.org/cache/epub/84/pg84.txt', Frankenstein, tokenize the data like we did before with NLTK, and (1) show the most common 20 tokens (bag-of-words), (2) create a histogram of the average length of words in this book.\n",
        "\n",
        "# Once you print it, you will realize that some words are just verbs or grammatical bindings. Modify the code in a way the top 10 makes sense for YOU.\n",
        "\n",
        "# You can (optionally) try to do it with WordNetLemmatizer, and then compare the results.\n",
        "\n",
        "# Your code here..."
      ],
      "metadata": {
        "id": "gCFxzHPSjzQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your own version of regular expression tokenizer. Here we specify that it should only look for words.\n",
        "timur = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Let's do the processing.\n",
        "words = timur.tokenize(adam_smith)\n",
        "\n",
        "# What words do you want to exclude?\n",
        "custom_exclusions = ['upon', 'may', 'therefore', 'one', 'much', 'must', '0', 'whole']\n",
        "\n",
        "# Just add them to the stopwords list.\n",
        "all_exclusions = stopwords.words('english') + custom_exclusions\n",
        "\n",
        "# Use the filter.\n",
        "bayern = [word for word in words if word.lower() not in all_exclusions]\n",
        "\n",
        "# Okay, one last time counting it all, let's see what we have now.\n",
        "wealth_counts_filtered = Counter(bayern)\n",
        "wealth_counts_filtered.most_common(20)\n",
        "\n",
        "# Can we somehow make this better?"
      ],
      "metadata": {
        "id": "OGaUXBDvDqoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to SpaCy and further libraries"
      ],
      "metadata": {
        "id": "Y9t70WDcj2NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following will take a while.\n",
        "# Introducing Spacy.\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(adam_smith)\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text)\n",
        "\n",
        "# You can do this step in NLTK as well, but spacy has some features that are way more neat. Also, it has some extra categories! NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT"
      ],
      "metadata": {
        "id": "4IDdHj412s59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "HaM80U0f3MCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  if ent.label_ == 'MONEY':\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "  elif  ent.label_ == 'CARDINAL':\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "XIzku34M3hiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpe_list = list()\n",
        "for ent in doc.ents:\n",
        "  if ent.label_ == 'GPE':\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "    gpe_list.append(ent.text)"
      ],
      "metadata": {
        "id": "BhkdEK6M4D24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using regular expressions, let's seach if mentions Turkey.\n",
        "import re\n",
        "for gpe in gpe_list:\n",
        "    if re.search(\"^Tur\", gpe):\n",
        "        print(gpe)"
      ],
      "metadata": {
        "id": "vjaFRTL14XLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What about the U.S.?\n",
        "for gpe in gpe_list:\n",
        "    if re.search(\"^U\", gpe):\n",
        "        print(gpe)"
      ],
      "metadata": {
        "id": "iP8H-jzk5lV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  if ent.label_ == 'ORG':\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "gTz6SjKg8lpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  if ent.label_ == 'PERSON':\n",
        "    print(ent.text, ent.start_char)"
      ],
      "metadata": {
        "id": "x0HDXTyt9JdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interesting that Adam Smith cites Caesar in his book. Let's see in what type of context that is...\n",
        "print(adam_smith[416500:417150])"
      ],
      "metadata": {
        "id": "8x0baqRt-w-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(' '.join(gpe_list))\n",
        "\n",
        "# Plotting the WordCloud\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nl_C2UBkm7lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment 2: Using \"https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/ricardo.txt\", follow similar steps to The Wealth of Nations and produce a story from the book.\n",
        "\n",
        "# Your code here..."
      ],
      "metadata": {
        "id": "M9DWdox6p00_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ricardo = requests.get('https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/ricardo.txt').text\n",
        "doc = nlp(ricardo)"
      ],
      "metadata": {
        "id": "idWZZu1EN474"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpe_list = list()\n",
        "for ent in doc.ents:\n",
        "  if ent.label_ == '':\n",
        "    gpe_list.append(ent.text)\n",
        "\n",
        "exclusion = ['Smith']\n",
        "gpe_list = [gpe for gpe in gpe_list if gpe not in exclusion]\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(' '.join(gpe_list))\n",
        "# Plotting the WordCloud\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m8QZ-fWKOdp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}