{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Identification & Classification\n",
        "\n",
        "A fundamental concept of topic identification can be done by using bag-of-words.Simply by counting our visualizing which words are repeated the most often, we can understand the main idea of the text.\n",
        "\n",
        "Comparison could be done similarly as well. If you remember our application of word clouds, you will see that the text from Adam Smith and David Ricardo had quite similar wordclouds. Since in real life we use much more complex resources, it is essential to learn advanced methods."
      ],
      "metadata": {
        "id": "nNGkEVqghs_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW7KclhNhqgw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Some extra knowledge for the computer, so it knows where the sentences are.\n",
        "nltk.download('stopwords') # Same, but this time it will know the english stopwords like 'the' and 'and'.\n",
        "from  nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remember the concept of bag-of-words, using Adam Smith and Ricardo.\n",
        "wealth_of_nations = requests.get(\"https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/The_Wealth_of_Nations_Volume_1_Cleaned.txt\").text\n",
        "ricardo = requests.get('https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/ricardo.txt').text"
      ],
      "metadata": {
        "id": "PqR98KxDltvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam Smith.\n",
        "# Tokenize the text.\n",
        "unique_tokens = word_tokenize(wealth_of_nations)\n",
        "\n",
        "# Lowercase all tokens so that there are no confusions. (i.e. 'The' vs. 'the')\n",
        "lower_tokens = [token.lower() for token in unique_tokens]\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "custom_exclusions = ['upon', 'may', 'therefore', 'one', 'much', 'must', '0', 'whole','would']\n",
        "all_exclusions = stopwords.words('english') + custom_exclusions\n",
        "filtered_words = [word for word in alpha_only if word not in all_exclusions]\n",
        "\n",
        "# Call the counter on our unique_tokens.\n",
        "adam_smith_counts = Counter(filtered_words)\n",
        "\n",
        "# What are the most common tokens?\n",
        "adam_smith_counts.most_common(5)"
      ],
      "metadata": {
        "id": "pHkmpMGqmHDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ricardo.\n",
        "# Tokenize the text.\n",
        "unique_tokens_r = word_tokenize(ricardo)\n",
        "\n",
        "# Lowercase all tokens so that there are no confusions. (i.e. 'The' vs. 'the')\n",
        "lower_tokens = [token.lower() for token in unique_tokens_r]\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "custom_exclusions = ['upon', 'may', 'therefore', 'one', 'much', 'must', '0', 'whole','would']\n",
        "all_exclusions = stopwords.words('english') + custom_exclusions\n",
        "filtered_words = [word for word in alpha_only if word not in all_exclusions]\n",
        "\n",
        "# Call the counter on our unique_tokens.\n",
        "ricardo_counts = Counter(filtered_words)\n",
        "\n",
        "# What are the most common tokens?\n",
        "ricardo_counts.most_common(5)"
      ],
      "metadata": {
        "id": "EiNoWL78qMUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(document):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(document.lower())  # Convert to lower case and tokenize\n",
        "    # Remove stopwords and punctuation, then lemmatize and stem (you can add stemmer.stem())\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in all_exclusions]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "85liH3DOryBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini Assignment 0: Preprocess the Smith and Ricardo books with the function above, look at the differences.\n",
        "\n",
        "a = preprocess_text(wealth_of_nations)\n",
        "a1 = Counter(a)\n",
        "a1.most_common(10)"
      ],
      "metadata": {
        "id": "d7_OexrXE1bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(preprocess_text(ricardo)).most_common(10)"
      ],
      "metadata": {
        "id": "UnLjbKLRH7gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text2(document):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(document.lower())  # Convert to lower case and tokenize\n",
        "    # Remove stopwords and punctuation, then lemmatize and stem (you can add stemmer.stem())\n",
        "    tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens if word.isalnum() and word not in all_exclusions]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "p0RRW_k1HxfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(preprocess_text(wealth_of_nations)).most_common(10))\n",
        "print(Counter(preprocess_text(ricardo)).most_common(10))"
      ],
      "metadata": {
        "id": "gVCYSelTIBig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(preprocess_text2(wealth_of_nations)).most_common(10))\n",
        "print(Counter(preprocess_text2(ricardo)).most_common(10))"
      ],
      "metadata": {
        "id": "RKpf5QqjILC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Gensim - https://pypi.org/project/gensim/\n",
        "\n",
        "For the core concepts of gensim - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
        "\n",
        "Gensim is an open-source Python library. It's particularly well-suited for applications like topic modeling, document similarity analysis, and building word embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "_R8ivtCpPhJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "# Let's define our data sources.\n",
        "data = [wealth_of_nations, ricardo]\n",
        "\n",
        "# We want to preprocess our data as specified by our function above.\n",
        "texts = [preprocess_text(doc) for doc in data]"
      ],
      "metadata": {
        "id": "6LDpHi1QnuP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What we essentially do is create dictionaries and corpora, so that our machine understands the text numerically.\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# A basic corpus can be created with bag-of-words, like our approach during the last weeks.\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Here is the crux of the whole thing. https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "lda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=25)\n",
        "\n",
        "# Print the topics of the model.\n",
        "topics = lda_model.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "# What about the topics in Ricardo?\n",
        "lda_model.get_document_topics(corpus[1])"
      ],
      "metadata": {
        "id": "b9A_vGyXrdMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini-Assingment: What other models could we use? You should find this out and apply two of those different models here. Let's see what you find out.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############# Your code here.\n",
        "\n",
        "lsimodel = models.LsiModel(corpus, num_topics=3, id2word=dictionary)\n",
        "lsimodel.print_topics(3,4)"
      ],
      "metadata": {
        "id": "P28yyXnJwwo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models.LsiModel(corpus, num_topics=3, id2word=dictionary).show_topics(2,4)"
      ],
      "metadata": {
        "id": "oj3c6aloP-KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume that we have a big corpus on our hands. And now we would like to find out how another document is related to the topics in this corpus.\n",
        "# Let's learn by applying it.\n",
        "\n",
        "# Time forward to 20th century.\n",
        "keynes = requests.get('https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/keynes.txt').text\n",
        "\n",
        "# Preprocess.\n",
        "general_theory = preprocess_text(keynes)\n",
        "\n",
        "# We convert the text from Keynes to BOW.\n",
        "general_theory_corpus = dictionary.doc2bow(general_theory)\n",
        "\n",
        "# And we ask for topics, easy as that!\n",
        "general_theory_topics = lda_model.get_document_topics(general_theory_corpus)\n",
        "print(general_theory_topics)"
      ],
      "metadata": {
        "id": "nMLnB82NuNs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This could be a bit complicated to read, but here is a function to solve our problem.\n",
        "def get_document_topics(lda_model, corpus):\n",
        "    topics = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
        "    topic_details = [(topic, f\"{' '.join(word for word, _ in lda_model.show_topic(topic, topn=10))}\", prob) for topic, prob in topics]\n",
        "    return topic_details\n",
        "\n",
        "general_theory_topics = get_document_topics(lda_model, general_theory_corpus)\n",
        "\n",
        "# Print the topic distribution with words for the new document\n",
        "for topic_num, words, prob in general_theory_topics:\n",
        "    print(f\"Topic {topic_num}: {words} - Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "9R0_cNLcuvap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What if we include something mostly unrelated to the corpus?\n",
        "frankenstein = requests.get('https://www.gutenberg.org/cache/epub/84/pg84.txt').text\n",
        "\n",
        "frankenstein_text = preprocess_text(frankenstein)\n",
        "frankenstein_corpus = dictionary.doc2bow(frankenstein_text)\n",
        "frankenstein_topics = lda_model.get_document_topics(frankenstein_corpus)\n",
        "frankenstein_topics = get_document_topics(lda_model, frankenstein_corpus)\n",
        "\n",
        "for topic_num, words, prob in frankenstein_topics:\n",
        "    print(f\"Topic {topic_num}: {words} - Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "a20RvALyv6oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tf-idf** stands for term-frequncy - inverse document frequency. It is an NLP model that helps you determine the most important words in each document in the corpus. The idea behind tf-idf is that each corpus might have more shared words than just stopwords (think about economics books). These common words are like stopwords and should be removed or at least down-weighted in importance.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
      ],
      "metadata": {
        "id": "ZC3synVT0QKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import TfidfModel\n",
        "\n",
        "# Initiating the model.\n",
        "tfidf = TfidfModel(corpus)"
      ],
      "metadata": {
        "id": "2YZqvXBezUDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which weights it gets from Adam Smith.\n",
        "tfidf_weights = tfidf[corpus[1]]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:500])"
      ],
      "metadata": {
        "id": "k6REbz2g01c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "metadata": {
        "id": "q7ZBlY4vzYR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language Detection\n",
        "# Examples can be given from our movie review dataset.\n",
        "movie_reviews = pd.read_csv('https://raw.githubusercontent.com/timuroeztuerk/data-science-lecture-S24/main/Datasets/imdb_review_dataset.csv')\n",
        "movie_reviews.head()"
      ],
      "metadata": {
        "id": "T2DgbBRw_G16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see an example.\n",
        "movie_reviews.review[12312]"
      ],
      "metadata": {
        "id": "t6BFgktY6yZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to install this package to our environment, since Google does not have it already.\n",
        "!pip install langdetect\n",
        "# Classic import.\n",
        "from langdetect import detect_langs"
      ],
      "metadata": {
        "id": "i62kE5DX7F9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting the language is (as you can imagine) very easy. We simply call detect_langs on a string.\n",
        "print(detect_langs(movie_reviews.review[12123]))"
      ],
      "metadata": {
        "id": "nRXbyLok7RjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application Session 2.1: Add a new column to our dataset, that specifies which language(s) the review is in.\n",
        "\n",
        "# 2.2. Now show some descriptive statistics. What are the minimum and maximum probabilities for english (a)? Are there any multilangual reviews (b)?\n",
        "\n",
        "# 2.3. Lastly, using the TextBlob package, add a sentiment column, and specify 4 sentiments; very positive, positive, negative, very negative. For this you have to do a little research.\n",
        "\n",
        "\n",
        "#################################### Your code here...\n",
        "\n"
      ],
      "metadata": {
        "id": "6G56Ex7t7-pi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}